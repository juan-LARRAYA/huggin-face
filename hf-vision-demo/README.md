# ğŸ¤– HF Vision Demo - Top 3 Computer Vision Models

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.44+-yellow.svg)](https://huggingface.co/transformers)
[![License](https://img.shields.io/badge/License-MIT-red.svg)](LICENSE)

An interactive real-time demo showcasing the **3 most popular computer vision models** from Hugging Face, running directly in your browser using your webcam.

## âœ¨ Features

- ğŸ¥ **Real-time webcam processing**
- ğŸš€ **3 state-of-the-art AI models** in one application
- ğŸ¨ **Modern, responsive web interface**
- ğŸ”§ **Easy setup and deployment**
- ğŸ“± **Cross-platform compatibility**
- ğŸ¯ **Zero-shot classification** with custom prompts

## ğŸ¯ Included Models

### 1. **CLIP** (openai/clip-vit-base-patch32)

![CLIP](https://img.shields.io/badge/Downloads-10M+-brightgreen)

- **What it does**: Zero-shot classification using text and images
- **Capabilities**: Classifies images using natural language descriptions
- **Use cases**: Content moderation, image search, flexible categorization
- **Example**: "a photo of a cat", "a person running", "Italian food"

### 2. **ViT** (google/vit-base-patch16-224)

![ViT](https://img.shields.io/badge/Downloads-5M+-brightgreen)

- **What it does**: Image classification using Vision Transformer
- **Capabilities**: Classifies images into 1000 ImageNet categories
- **Use cases**: Object recognition, product categorization, automated tagging
- **Example**: Identifies specific objects like "Golden Retriever", "laptop", "pizza"

### 3. **DETR** (facebook/detr-resnet-50)

![DETR](https://img.shields.io/badge/Downloads-2M+-brightgreen)

- **What it does**: Object detection using Detection Transformer
- **Capabilities**: Detects and localizes multiple objects with bounding boxes
- **Use cases**: Autonomous vehicles, security systems, quality control
- **Example**: Finds people, cars, animals and draws boxes around them

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.9+** (3.10+ recommended)
- **Webcam** (built-in or external)
- **4GB+ RAM** (8GB+ recommended for optimal performance)
- **Modern web browser** (Chrome, Firefox, Safari, Edge)

### Installation

1. **Clone the repository**:

   ```bash
   git clone https://github.com/juan-LARRAYA/huggin-face.git
   cd huggin-face/hf-vision-demo
   ```

2. **Create virtual environment**:

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Start the backend server**:

   ```bash
   uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
   ```

5. **Open the frontend**:
   - Open `frontend/index.html` in your browser
   - Or serve with a local server: `python -m http.server 3000`

### First Run

âš ï¸ **Important**: The first execution will download the models (~2GB total). This may take several minutes depending on your internet connection.

## ğŸ® How to Use

1. **Allow camera access** when prompted by your browser
2. **Try each model**:

   - ğŸ” **ViT Classifier**: Classifies what the camera sees
   - ğŸ¯ **CLIP Zero-Shot**: Classifies using predefined categories
   - ğŸ“¦ **DETR Object Detection**: Detects and marks objects with boxes
   - ğŸ¨ **CLIP Custom**: Use your own text prompts

3. **For CLIP Custom**: Write comma-separated prompts like:
   ```
   a happy person, an animal, technology, delicious food
   ```

## ğŸ“Š Interface Features

- ğŸ¨ **Modern UI** with gradients and visual effects
- âš¡ **Real-time processing** using your webcam
- ğŸ¯ **Interactive visualization** of detections with colored boxes
- ğŸ“ˆ **Results sorted by confidence**
- ğŸ“± **Responsive design** that adapts to different screens
- ğŸ”„ **Live updates** as you move objects in front of the camera

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   FastAPI       â”‚    â”‚  Hugging Face   â”‚
â”‚   (HTML/JS)     â”‚â—„â”€â”€â–ºâ”‚   Backend       â”‚â—„â”€â”€â–ºâ”‚    Models       â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Camera feed   â”‚    â”‚ â€¢ Image proc.   â”‚    â”‚ â€¢ CLIP          â”‚
â”‚ â€¢ UI controls   â”‚    â”‚ â€¢ Model calls   â”‚    â”‚ â€¢ ViT           â”‚
â”‚ â€¢ Visualization â”‚    â”‚ â€¢ API endpoints â”‚    â”‚ â€¢ DETR          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Technical Details

- **Model loading**: Automatic download on first run (~2GB total)
- **GPU support**: Automatically detected if available (CUDA/MPS)
- **Memory usage**: 4-8GB RAM depending on models loaded
- **Performance**: ~1-3 seconds per inference depending on hardware
- **Supported formats**: JPEG, PNG images from webcam

## ğŸ› Troubleshooting

### Common Issues

| Problem                    | Solution                                                                                                     |
| -------------------------- | ------------------------------------------------------------------------------------------------------------ |
| ğŸ¥ **Camera not working**  | â€¢ Check browser permissions<br>â€¢ Try a different browser<br>â€¢ Ensure camera isn't used by other apps         |
| ğŸŒ **Slow inference**      | â€¢ First run downloads models (normal)<br>â€¢ Close other applications<br>â€¢ Use GPU if available                |
| ğŸ’¾ **Out of memory**       | â€¢ Close other applications<br>â€¢ Use a machine with more RAM<br>â€¢ Restart the backend                         |
| ğŸŒ **Connection error**    | â€¢ Ensure backend is running on port 8000<br>â€¢ Check browser console for errors<br>â€¢ Verify firewall settings |
| ğŸ“¦ **Model loading fails** | â€¢ Check internet connection<br>â€¢ Clear Hugging Face cache<br>â€¢ Restart with `--reload`                       |

### Performance Tips

- **GPU Acceleration**: Install CUDA (NVIDIA) or use Apple Silicon for better performance
- **Memory Management**: Close unnecessary applications before running
- **Browser Choice**: Chrome and Firefox typically perform best
- **Network**: Stable internet connection required for first-time model downloads

## ğŸ¯ Example Use Cases

### ğŸ¥ Healthcare

```
# CLIP prompts for medical imaging
"normal chest X-ray, pneumonia, fracture, medical equipment"
```

### ğŸ›’ E-commerce

```
# Product categorization with ViT
Automatically classify product images into categories
```

### ğŸš— Autonomous Vehicles

```
# DETR for object detection
Detect pedestrians, vehicles, traffic signs, road obstacles
```

### ğŸ“± Social Media

```
# Content moderation with CLIP
"appropriate content, inappropriate content, violence, safe for work"
```

### ğŸ­ Quality Control

```
# Manufacturing defect detection
"defective product, normal product, damaged packaging"
```

## ğŸ“Š Model Comparison

| Model    | Size   | Speed  | Accuracy  | Best For                           |
| -------- | ------ | ------ | --------- | ---------------------------------- |
| **CLIP** | ~600MB | Medium | High      | Flexible classification, zero-shot |
| **ViT**  | ~350MB | Fast   | Very High | Precise object recognition         |
| **DETR** | ~170MB | Medium | High      | Object detection, localization     |

## ğŸ› ï¸ Development

### API Endpoints

- `GET /` - API information and available models
- `POST /classify` - ViT image classification
- `POST /clip` - CLIP zero-shot classification
- `POST /detect` - DETR object detection
- `POST /clip-custom` - CLIP with custom prompts

### Project Structure

```
hf-vision-demo/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py          # FastAPI backend server
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html       # Web interface
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ demo_script.py      # Demo and testing script
â””â”€â”€ README.md           # This file
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Hugging Face](https://huggingface.co/) for providing the amazing models and transformers library
- [OpenAI](https://openai.com/) for CLIP
- [Google](https://github.com/google-research/vision_transformer) for Vision Transformer
- [Facebook Research](https://github.com/facebookresearch/detr) for DETR

## ğŸ“ Support

If you encounter any issues or have questions:

1. Check the [troubleshooting section](#-troubleshooting)
2. Look at existing [GitHub issues](https://github.com/juan-LARRAYA/huggin-face/issues)
3. Create a new issue with detailed information

---

**Made with â¤ï¸ using the top 3 computer vision models from Hugging Face** ğŸ¤—
